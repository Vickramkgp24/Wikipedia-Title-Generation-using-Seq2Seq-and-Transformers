{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac61469f",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9a5dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "import torch.optim as optim\n",
    "from rouge import Rouge\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b994b8",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1ee524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Load Data =====================\n",
    "dir = 'part_A_outputs'\n",
    "df_train = pd.read_csv(dir + '/train.csv').fillna('')\n",
    "df_val = pd.read_csv(dir + '/validation.csv').fillna('')\n",
    "df_test = pd.read_csv(dir + '/test.csv').fillna('')\n",
    "\n",
    "\n",
    "\n",
    "# ===================== Vocabulary =====================\n",
    "def build_vocab(texts, min_freq=0.01):\n",
    "    special_tokens = ['<pad>', '<bos>', '<eos>', '<unk>']\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        counter.update(text.split())\n",
    "    threshold = len(texts) * min_freq\n",
    "    tokens = [t for t, f in counter.items() if f >= threshold]\n",
    "    vocab = {word: idx for idx, word in enumerate(special_tokens + tokens)}\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(df_train['text'])\n",
    "print(len(vocab))\n",
    "inv_vocab = {i: w for w, i in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e34d1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db095cd",
   "metadata": {},
   "source": [
    "### Dataset Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53a04c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Dataset =====================\n",
    "class TitleDataset(Dataset):\n",
    "    def __init__(self, texts, titles, vocab, max_len):\n",
    "        self.texts = texts\n",
    "        self.titles = titles\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def encode(self, sentence, add_tokens=False):\n",
    "        tokens = sentence.split()\n",
    "        idxs = [self.vocab.get(tok, self.vocab['<unk>']) for tok in tokens]\n",
    "        if add_tokens:\n",
    "            idxs = [self.vocab['<bos>']] + idxs + [self.vocab['<eos>']]\n",
    "        idxs = idxs[:self.max_len]\n",
    "        return torch.tensor(idxs + [self.vocab['<pad>']] * (self.max_len - len(idxs)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.encode(self.texts[idx]), self.encode(self.titles[idx], add_tokens=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45043a7",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1169bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, vocab, emb_dim=300, hidden_dim=300):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), emb_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def load_embeddings(self, weight_matrix):\n",
    "        self.embedding.weight.data.copy_(weight_matrix)\n",
    "        self.embedding.weight.requires_grad = False  # Optional: Freeze embeddings\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, hidden = self.gru(embedded)\n",
    "        hidden = torch.cat((hidden[-2], hidden[-1]), dim=1).unsqueeze(0)\n",
    "        return outputs, hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6cd5e1",
   "metadata": {},
   "source": [
    "### Hierarchical Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049ed2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class HierarchicalEncoderRNN(nn.Module):\n",
    "    def __init__(self, vocab, emb_dim=300, hidden_dim=300, sent_len=30):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.sent_len = sent_len\n",
    "        vocab_size = len(vocab)\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        \n",
    "        # Word-level GRU (process each sentence)\n",
    "        self.word_gru = nn.GRU(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Sentence-level GRU (process each sentence representation)\n",
    "        self.sent_gru = nn.GRU(hidden_dim * 2, hidden_dim, batch_first=True, bidirectional=True)\n",
    "    \n",
    "    def load_embeddings(self, weight_matrix):\n",
    "        self.embedding.weight.data.copy_(weight_matrix)\n",
    "        self.embedding.weight.requires_grad = False  # Optional: Freeze embeddings\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, total_seq_len]\n",
    "        batch_size, total_seq_len = x.shape\n",
    "        if total_seq_len < self.sent_len:\n",
    "            pad_len = self.sent_len - total_seq_len\n",
    "            x = torch.cat([x, torch.zeros(batch_size, pad_len, dtype=torch.long, device=x.device)], dim=1)\n",
    "            total_seq_len = self.sent_len\n",
    "\n",
    "        max_tokens = (total_seq_len // self.sent_len) * self.sent_len\n",
    "        x = x[:, :max_tokens]\n",
    "        num_sents = max_tokens // self.sent_len\n",
    "\n",
    "\n",
    "        # Reshape to [B, num_sents, sent_len]\n",
    "        x = x.view(batch_size, num_sents, self.sent_len)\n",
    "\n",
    "        # Embed: [B, num_sents, sent_len, emb_dim]\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # Reshape to merge sentences into batch: [B * num_sents, sent_len, emb_dim]\n",
    "        embedded = embedded.view(batch_size * num_sents, self.sent_len, self.emb_dim)\n",
    "\n",
    "        # Word-level GRU\n",
    "        word_outputs, word_hidden = self.word_gru(embedded)\n",
    "        # Concatenate forward and backward hidden states: [B * num_sents, hidden_dim * 2]\n",
    "        word_hidden = torch.cat((word_hidden[-2], word_hidden[-1]), dim=1)\n",
    "\n",
    "        # Reshape back to sentence-level: [B, num_sents, hidden_dim * 2]\n",
    "        sentence_reps = word_hidden.view(batch_size, num_sents, self.hidden_dim * 2)\n",
    "\n",
    "        # Sentence-level GRU\n",
    "        sent_outputs, sent_hidden = self.sent_gru(sentence_reps)\n",
    "\n",
    "        # Combine last forward and backward hidden states\n",
    "        sent_hidden = torch.cat((sent_hidden[-2], sent_hidden[-1]), dim=1).unsqueeze(0)  # [1, B, H*2]\n",
    "\n",
    "        return sent_outputs, sent_hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd1e0ff",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf93812",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, vocab, emb_dim=300, hidden_dim=600):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), emb_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(emb_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, len(vocab))\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x.unsqueeze(1))\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        logits = self.fc(output.squeeze(1))\n",
    "        return logits, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c507ab1",
   "metadata": {},
   "source": [
    "### Decoder2RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6766c72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder2RNN(nn.Module):\n",
    "    def __init__(self, vocab, emb_dim=300, hidden_dim=600):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), emb_dim, padding_idx=0)\n",
    "        self.gru1 = nn.GRU(emb_dim, hidden_dim, batch_first=True)\n",
    "        self.gru2 = nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, len(vocab))\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x.unsqueeze(1))\n",
    "        out1, hidden1 = self.gru1(x, hidden)\n",
    "        out2, hidden2 = self.gru2(out1, hidden)\n",
    "        logits = self.fc(out2.squeeze(1))\n",
    "        return logits, hidden2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a925e4",
   "metadata": {},
   "source": [
    "### Seq2Seq2 class with Beam Search code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f67074",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqRNN(nn.Module):\n",
    "    def __init__(self, vocab, max_len, emb_dim=300, hidden_dim=300,\n",
    "                 use_hier=False, use_dec2=False, glove_path=None):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # Encoder\n",
    "        if use_hier:\n",
    "            self.encoder = HierarchicalEncoderRNN(vocab, emb_dim, hidden_dim)\n",
    "        else:\n",
    "            self.encoder = EncoderRNN(vocab, emb_dim, hidden_dim)\n",
    "\n",
    "        # Load GloVe if path given\n",
    "        if glove_path:\n",
    "            glove_matrix = self.load_glove_weights(glove_path, emb_dim)\n",
    "            self.encoder.load_embeddings(glove_matrix)\n",
    "\n",
    "        # Decoder\n",
    "        if use_dec2:\n",
    "            self.decoder = Decoder2RNN(vocab, emb_dim, hidden_dim * 2)\n",
    "        else:\n",
    "            self.decoder = DecoderRNN(vocab, emb_dim, hidden_dim * 2)\n",
    "\n",
    "    def load_glove_weights(self, path, emb_dim):\n",
    "        weights = torch.randn(len(self.vocab), emb_dim)\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                word = parts[0]\n",
    "                if word in self.vocab:\n",
    "                    idx = self.vocab[word]\n",
    "                    weights[idx] = torch.tensor(list(map(float, parts[1:])), device=device)\n",
    "        return weights.to(device)\n",
    "\n",
    "    def forward(self, src, tgt=None, teacher_forcing=True, beam_search=False, beam_width=3):\n",
    "        if beam_search:\n",
    "            return self.beam_decode(src, beam_width)\n",
    "        else:\n",
    "            return self.greedy_decode(src, tgt, teacher_forcing)\n",
    "\n",
    "    def greedy_decode(self, src, tgt=None, teacher_forcing=True):\n",
    "        _, hidden = self.encoder(src)\n",
    "        input_token = torch.full((src.size(0),), self.vocab['<bos>'], dtype=torch.long, device=src.device)\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(self.max_len):\n",
    "            out, hidden = self.decoder(input_token, hidden)\n",
    "            outputs.append(out.unsqueeze(1))\n",
    "            if teacher_forcing and tgt is not None and t < tgt.size(1):\n",
    "                input_token = tgt[:, t]\n",
    "            else:\n",
    "                input_token = out.argmax(1)\n",
    "\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "    def beam_decode(self, src, beam_width):\n",
    "        batch_size = src.size(0)\n",
    "        assert batch_size == 1, \"Beam search supports batch size 1 for now.\"\n",
    "        \n",
    "        # Get encoder output\n",
    "        _, hidden = self.encoder(src)\n",
    "        \n",
    "        # Initialize beam\n",
    "        # Each sequence contains: [token_ids, score, hidden_state]\n",
    "        sequences = [[[self.vocab['<bos>']], 0.0, hidden]]\n",
    "        finished_sequences = []\n",
    "        \n",
    "        # Beam search\n",
    "        for _ in range(self.max_len - 1):  # -1 because we already added <bos>\n",
    "            all_candidates = []\n",
    "            \n",
    "            # Expand each current sequence\n",
    "            for seq, score, h in sequences:\n",
    "                # Get the last token in sequence\n",
    "                last_token = seq[-1]\n",
    "                if last_token == self.vocab['<eos>']:\n",
    "                    # If this sequence has ended, keep it for later\n",
    "                    finished_sequences.append([seq, score, h])\n",
    "                    continue\n",
    "                    \n",
    "                # Convert last token to tensor for decoder input\n",
    "                input_token = torch.tensor([last_token], device=src.device)\n",
    "                \n",
    "                # Get decoder output\n",
    "                out, h_new = self.decoder(input_token, h)\n",
    "                log_probs = torch.nn.functional.log_softmax(out, dim=1)\n",
    "                \n",
    "                # Get top k candidates\n",
    "                topk_log_probs, topk_idxs = torch.topk(log_probs, beam_width)\n",
    "                \n",
    "                # Create new candidate sequences\n",
    "                for i in range(beam_width):\n",
    "                    next_token = topk_idxs[0][i].item()\n",
    "                    next_score = score + topk_log_probs[0][i].item()\n",
    "                    candidate = [seq + [next_token], next_score, h_new]\n",
    "                    all_candidates.append(candidate)\n",
    "            \n",
    "            # If all sequences have finished or we've run out of candidates\n",
    "            if not all_candidates:\n",
    "                break\n",
    "                \n",
    "            # Select top beam_width sequences\n",
    "            ordered = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)\n",
    "            sequences = ordered[:beam_width]\n",
    "            \n",
    "            # Early stopping if all sequences have generated <eos>\n",
    "            if all(seq[-1] == self.vocab['<eos>'] for seq, _, _ in sequences):\n",
    "                finished_sequences.extend(sequences)\n",
    "                break\n",
    "        \n",
    "        # Add any unfinished sequences to the finished list\n",
    "        finished_sequences.extend(sequences)\n",
    "        \n",
    "        # Sort finished sequences by score\n",
    "        finished_sequences = sorted(finished_sequences, key=lambda tup: tup[1], reverse=True)\n",
    "        \n",
    "        # Get best sequence\n",
    "        best_seq = finished_sequences[0][0]\n",
    "        \n",
    "        # Convert token IDs to one-hot representation for each position\n",
    "        vocab_size = len(self.vocab)\n",
    "        output_tensor = torch.zeros(1, len(best_seq), vocab_size, device=src.device)\n",
    "        \n",
    "        # Fill in the one-hot positions\n",
    "        for t, token_id in enumerate(best_seq):\n",
    "            output_tensor[0, t, token_id] = 1.0\n",
    "            \n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2378e6",
   "metadata": {},
   "source": [
    "### Training \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee4c327",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "glove_path = os.path.join('.', 'glove_vectors', 'glove.6B.300d.txt')\n",
    "max_lens = [8]\n",
    "batch_sizes = [32]\n",
    "\n",
    "# to activate hierencoder, decoder2rnn and glove_path, make changes here.\n",
    "def get_model(vocab, max_len, emb_dim=300, hidden_dim=300):\n",
    "    model = Seq2SeqRNN(\n",
    "        vocab, max_len=max_len, emb_dim=emb_dim, hidden_dim=hidden_dim,\n",
    "        use_hier=False, use_dec2=False, glove_path=None\n",
    "    ).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    return model, optimizer\n",
    "\n",
    "def evaluate(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x, y, teacher_forcing=False)\n",
    "            out = out[:, :y.shape[1], :]  # Ensure match\n",
    "            loss = criterion(out.reshape(-1, out.shape[-1]), y.reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "best_overall_loss = float('inf')\n",
    "best_model_state_dict = None  # Save best model in memory\n",
    "results = []\n",
    "\n",
    "for max_len_val, batch_size_val in product(max_lens, batch_sizes):\n",
    "    print(f\"\\nðŸŸ¦ Training for max_len={max_len_val}, batch_size={batch_size_val}\")\n",
    "    \n",
    "    train_loader = DataLoader(TitleDataset(df_train['text'], df_train['title'], vocab, max_len_val),\n",
    "                              batch_size=batch_size_val, shuffle=True)\n",
    "    val_loader = DataLoader(TitleDataset(df_val['text'], df_val['title'], vocab, max_len_val),\n",
    "                            batch_size=batch_size_val)\n",
    "\n",
    "    model, optimizer = get_model(vocab, max_len_val)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])\n",
    "    # Early stopping\n",
    "    max_patience = 3\n",
    "    patience_counter = 0\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x, y, teacher_forcing=True)\n",
    "            out = out[:, :y.shape[1], :]\n",
    "            loss = criterion(out.reshape(-1, out.shape[-1]), y.reshape(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        avg_val_loss = evaluate(model, val_loader, criterion)\n",
    "        print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= max_patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    # Save the best model across ALL configs\n",
    "    if best_val_loss < best_overall_loss:\n",
    "        best_overall_loss = best_val_loss\n",
    "        best_model_state_dict = model.state_dict()  # Store weights\n",
    "\n",
    "    results.append({\n",
    "        'max_len': max_len_val,\n",
    "        'batch_size': batch_size_val,\n",
    "        'val_loss': best_val_loss,\n",
    "        'epoch': best_epoch\n",
    "    })\n",
    "\n",
    "    print(f\"âœ… Finished: max_len={max_len_val}, batch_size={batch_size_val}, best_val_loss={best_val_loss:.4f} at epoch {best_epoch}\")\n",
    "\n",
    "# Save only the best model globally\n",
    "if best_model_state_dict is not None:\n",
    "    torch.save(best_model_state_dict, 'best_seq2seq.pt')\n",
    "    print(\"ðŸŸ¢ Best model saved as 'best_seq2seq.pt'\")\n",
    "\n",
    "# ==== Plot Heatmap ====\n",
    "df_results = pd.DataFrame(results)\n",
    "heatmap_data = df_results.pivot(index='max_len', columns='batch_size', values='val_loss')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".4f\", cmap=\"viridis\")\n",
    "plt.title(\"Validation Loss Heatmap\")\n",
    "plt.ylabel(\"Max Sequence Length\")\n",
    "plt.xlabel(\"Batch Size\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af94e2ff",
   "metadata": {},
   "source": [
    "### Generating Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6e91fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return text.lower().strip()\n",
    "\n",
    "def generate_title(model, src_tensor, beam=False, beam_width=3):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Call the model with appropriate parameters\n",
    "        output = model(src_tensor, beam_search=beam, beam_width=beam_width)\n",
    "        \n",
    "        # Check if output is a tensor (from beam search) or a string\n",
    "        if isinstance(output, torch.Tensor):\n",
    "            # Convert tensor to tokens and then to string\n",
    "            output_ids = output.argmax(dim=-1)  # [batch_size, seq_len]\n",
    "            \n",
    "            # Convert token IDs to words\n",
    "            words = []\n",
    "            for i in range(output_ids.size(0)):\n",
    "                seq_words = [inv_vocab.get(idx.item(), '[UNK]') for idx in output_ids[i] \n",
    "                           if idx.item() not in [vocab['<bos>'], vocab['<eos>'], vocab['<pad>']]]\n",
    "                words.append(' '.join(seq_words))\n",
    "            \n",
    "            # Join all words (typically just one sequence)\n",
    "            result = words[0] if words else ''\n",
    "        else:\n",
    "            # If output is already a string, use it directly\n",
    "            result = output\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55887c2d",
   "metadata": {},
   "source": [
    "### Printing Rouge Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a98faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_seq2seq.pt'))\n",
    "\n",
    "generated_titles = []\n",
    "\n",
    "# Track ROUGE scores for each example\n",
    "individual_rouge_scores = []\n",
    "\n",
    "for i, row in df_test.iterrows():\n",
    "    input_text = row['text']\n",
    "    reference_title = row['title']\n",
    "    \n",
    "    if not input_text:\n",
    "        continue\n",
    "\n",
    "    # Convert input_text to tensor\n",
    "    tokens = input_text.split()  # Replace with your actual tokenizer logic\n",
    "    token_ids = [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
    "    src_tensor = torch.tensor(token_ids, dtype=torch.long).unsqueeze(0).to(device)  # shape: [1, seq_len]\n",
    "\n",
    "    # Generate title using beam search\n",
    "    generated = generate_title(model, src_tensor, beam=False, beam_width=3)\n",
    "    \n",
    "    # Safely check if the generated text is empty\n",
    "    if not generated or generated.strip() == '':\n",
    "        generated = 'EMPTY'\n",
    "        \n",
    "    generated_titles.append(generated)\n",
    "\n",
    "# Evaluate using ROUGE\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(generated_titles, df_test['title'].tolist(), avg=True)\n",
    "print(\"\\nROUGE Evaluation:\")\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
