{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e04964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8839ba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# The correct packages to download\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Initialize resources\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64544d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NLTK POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ec1282",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "train_df, val_df = train_test_split(train_df, test_size = 500, random_state = 42)\n",
    "val_df.to_csv('validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516c1fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_numbers(text):\n",
    "    def replace_func(match):\n",
    "        num = match.group()\n",
    "        if 1900 <= int(num) <= 2100:\n",
    "            return num  # Keep years\n",
    "        else:\n",
    "            return 'NUM'\n",
    "    return re.sub(r'\\b\\d+\\b', replace_func, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f28ea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update your preprocess function to use stopwords_set instead of stopwords\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)  # remove non-ASCII\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)        # remove punctuation\n",
    "    text = normalize_numbers(text)             # replace numbers except years\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stopwords_set]  # Use stopwords_set here\n",
    "\n",
    "    # Lemmatize each token with POS tag\n",
    "    tagged = pos_tag(tokens)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(w, get_wordnet_pos(pos)) for w, pos in tagged]\n",
    "\n",
    "    return ' '.join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee1ff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preprocessing started..\")\n",
    "list1 = [train_df, test_df, val_df]\n",
    "for ent in list1:\n",
    "    ent['text'] = ent['text'].apply(preprocess)\n",
    "    ent['title'] = ent['title'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e752c659",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./part_A_outputs', exist_ok = True)\n",
    "train_df.to_csv('./part_A_outputs/train.csv', index = False)\n",
    "val_df.to_csv('./part_A_outputs/validation.csv', index = False)\n",
    "test_df.to_csv('./part_A_outputs/test.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7e3089",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_tokens = float('inf')\n",
    "max_tokens = float('-inf')\n",
    "print(type(train_df))\n",
    "for text1 in train_df['text']:\n",
    "    token_len = len(word_tokenize(text1))  \n",
    "    min_tokens = min(min_tokens, token_len)\n",
    "    max_tokens = max(max_tokens, token_len)\n",
    "\n",
    "print(f\"minimum length: {min_tokens}\\nmaximum length: {max_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b177e411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Compute token lengths for each text\n",
    "train_df = pd.read_csv(os.path.join('.','part_A_outputs','train.csv')) \n",
    "train_df['tokens'] = train_df['text'].apply(word_tokenize)\n",
    "train_df['token_len'] = train_df['tokens'].apply(len)\n",
    "\n",
    "# Step 2: Plot distribution (optional but useful)\n",
    "plt.figure(figsize=(10, 6))\n",
    "train_df['token_len'].hist(bins=50)\n",
    "plt.xlabel(\"Token Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Token Lengths in Training Data\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Calculate 85th percentile threshold\n",
    "threshold_85 = int(np.percentile(train_df['token_len'], 85))\n",
    "print(f\"85th Percentile Threshold: {threshold_85:.2f} tokens\")\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Filter rows within the 85th percentile\n",
    "train_df['text'] = train_df['tokens'].apply(lambda tokens : ' '.join(tokens[:threshold_85]))\n",
    "\n",
    "# Optional: Drop the helper column if no longer needed\n",
    "train_df.drop(columns=['token_len', 'tokens'], inplace=True)\n",
    "train_df.to_csv('./part_A_outputs/train.csv', index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
